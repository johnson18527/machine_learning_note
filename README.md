# 機器學習相關比筆記

## 特徵工程

### Normalization正規化

減少離群值隊預測果影響，針對資料數據，將資料數據(其中抹一列)縮放進指定範圍

- **方法**：Min-Max Normalization是將資料按比例原封不動的縮小至0到1的區間之中

- **好處**：提升模型的收斂速度-能減少梯度下降法的收斂時間

  

### Regularization正則化:

減緩模型過擬，針對模型本身(損失函數)，可限制權重大小

  

### Standardization標準化:

就是將訓練集中某一列數值特徵（假設是第i行）的值縮放成均值為0，方差為1的狀態

  

- 標準化更好保持了樣本間距。當樣本中有異常點時，歸一化有可能將正常的樣本“擠”

到一起去，如果不幸的是1和2的分類標籤還是相反的，那麼當用梯度下降來做分類模型訓練時，模型會需要更長的時間收斂

  

- 標準化更符合統計學假設。對一個數值特徵來說，很大可能它是服從常態分佈的

  

- 邏輯回歸必須要進行標準化嗎？如果你不用正則，那麼，標準化並不是必須的，如果你用正則，那麼標準化是必須的

不用正則時，損失函數只是僅僅在度量預測與真實的差距，加上正則後，我們的損失函數除了要度量上面的差距外，

還要度量參數值是否足夠小

  

- 如果不用正則，那麼標準化對邏輯回歸有什麼好處嗎？參數值的大小可以反應出不同特徵對樣本label的貢獻度，方便我們進

行特徵篩選

  

- 需要標準化數據的算法-`PCA` 、`SVM`、 `SGD`、`linear`/`logistic regression` 、`Kmeans`、`KNN`、`NN`（如果有用L1/L2 regularization）

- 涉及到距離有關的算法，或者聚類的話，都是需要先做變量標準化的。DecisionTree 、0/1取值的特徵、

- 基於平方損失的最小二乘法OLS，不需要歸一化。

## TF-IDF 
 - TF-IDF(t,d)=TF(t,d)*IDF(t)
 - TF(t,d):單詞t在文件d中出現的頻率

 - IDF(t)逆文件頻率:如果一個單詞在非常多文章裡都出現，那可能是一個比較通用的詞彙，對區分抹篇章特殊語意的貢獻較小，英此對權重做出一定懲罰

## 模型評估
### 準確度(accuracy)的侷限
當不同類別的樣本比例非常不均勻，占比較大的類別往往會成為引響準確度的主要因素。

### 精確度(precision)與召回率(recall)的權衡
precision與recall是既矛盾又統一的兩個指標，為了提升precision，分類器需要盡量在 "更有把握 " 時才把樣本預測為正樣本，但此時卻會因 "過於保守" 而漏掉很多沒有把握的正樣本，導致recall降低，而只用某點對應的
precision與recall是不能全面衡量模型的效能，只有透過P-R曲線的整體表現，才能對模型進行更全面的評估

### 平方根誤差的意外
一般情況下，RMSE可以良好的反應回歸模型預測值與真實質的偏離程度，但在真實情況下，如果存在偏離程度非常大的離群值，即使離群點數量非常少，也會讓RMSE指標便非常差。
針對這個問題可以從三個角度解決
- 如果認定離群點是雜訊點，就需要在資料預處理階段就把雜訊點濾掉
 - 如果不認定離群點是"雜訊點"，就需要進一步提升模型的預測能力，
將離群點產生的機制建模進去，
 - 使用更適合的指標來評估模型，如MAPE，相較於RMSE，MAPE把每個點的誤差都進行正規化，降低了離群點帶來的絕對誤差引響。

### ROC曲線
ROC曲線的橫軸為偽陽性率(FPR)，縱軸為真陽性率(TPR)，公式分別為FPR=FP/真實負樣本數，TPR=TP/真實正樣本數

### 如何計算AUC
AUC指ROC曲線底下面積大小，只需沿著ROC的橫軸做積分，AUC越大，說明越能把真正的正樣本排在最前面

## 最佳化演算法

### 訓練資料特別大時梯度下降法的問題與解決
梯度下降法每次更新模型參數時需尋訪所有訓練資料，當訓練資料特別大時，便要求極大的計算量，耗費很長的時間，且實際應用上基本不大可行，為了解決這個問題，**隨機梯度下降法**改用單個訓練樣本的損失來近似平均損失，單個訓練資料時，即可一次更新模型參數，大幅加跨收斂速度，而為了將低隨機梯度的變異數，進而使得迭代演算法更加穩定，並充分利用高度最佳化的矩陣運算操作，實際運用時會同時處理若干訓練資料，
該方法稱為**小批次梯度下降法**

## 演算法

### Lasso 
L1 norm:迴歸模型(可調整「誤差項」與「解釋變數量」)

### ElasticNet彈性網路回歸:
L1 norm與L2 norm綜合版

綜合了Ridge 懲罰項達到有效正規化優勢以及Lasso 懲罰項能夠進行變數挑選優勢，允許學習到一個只有少量引數是非零稀疏的模型鼓勵在高度相關變數的情況下的群體效應
能夠控制multicollinearity的的問題，能夠在p>n時執行回歸，並降低資料中過多的雜訊，以幫助我們將具有影響力的變數獨立出來且維持住模型正確率。

依舊有假設預測變數和目標變數需具有線性關係。雖然我們可以結合non-additive models(一種無母數回歸模型，non-parametric regression)交互作用，但當資料變數很多的時候，會是非常繁瑣與困難的

### KNeighborsRegressor:
預測值為k個最近鄰居的值的平均值

### DecisionTreeRegressor決策樹回歸:
計算熵與不純度

### Support Vector Regression:
讓多維分割線與被分割資料間距離最大化

### AdaBoost自我提升法:
前一個基本分類器分錯的樣本會得到加強，加權後的全體樣本再次被用來訓練下一個基本分類器

### GradientBoostingRegressor梯度提升法
先以弱模型預測。再將先前低度擬合的預測依次增加到集合中，確保錯誤修正

### 隨機森林
構建一棵樹的分裂節點的時候，隨機選取特徵


### 極端隨機森林
構建一棵樹的分裂節點的時候，不會任意地選取特徵；而是先隨機收集一部分特徵，然後利用信息熵和基尼不純性等指標調休最佳的節點特徵