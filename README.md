# 機器學習相關比筆記

## 特徵工程

### Normalization正規化

減少離群值隊預測果影響，針對資料數據，將資料數據(其中抹一列)縮放進指定範圍

- **方法**：Min-Max Normalization是將資料按比例原封不動的縮小至0到1的區間之中

- **好處**：提升模型的收斂速度-能減少梯度下降法的收斂時間

  

### Regularization正則化:

減緩模型過擬，針對模型本身(損失函數)，可限制權重大小

  

### Standardization標準化:

就是將訓練集中某一列數值特徵（假設是第i行）的值縮放成均值為0，方差為1的狀態

  

- 標準化更好保持了樣本間距。當樣本中有異常點時，歸一化有可能將正常的樣本“擠”

到一起去，如果不幸的是1和2的分類標籤還是相反的，那麼當用梯度下降來做分類模型訓練時，模型會需要更長的時間收斂

  

- 標準化更符合統計學假設。對一個數值特徵來說，很大可能它是服從常態分佈的

  

- 邏輯回歸必須要進行標準化嗎？如果你不用正則，那麼，標準化並不是必須的，如果你用正則，那麼標準化是必須的

不用正則時，損失函數只是僅僅在度量預測與真實的差距，加上正則後，我們的損失函數除了要度量上面的差距外，

還要度量參數值是否足夠小

  

- 如果不用正則，那麼標準化對邏輯回歸有什麼好處嗎？參數值的大小可以反應出不同特徵對樣本label的貢獻度，方便我們進

行特徵篩選

  

- 需要標準化數據的算法-`PCA` 、`SVM`、 `SGD`、`linear`/`logistic regression` 、`Kmeans`、`KNN`、`NN`（如果有用L1/L2 regularization）

- 涉及到距離有關的算法，或者聚類的話，都是需要先做變量標準化的。DecisionTree 、0/1取值的特徵、

- 基於平方損失的最小二乘法OLS，不需要歸一化。

## TF-IDF 
 - TF-IDF(t,d)=TF(t,d)*IDF(t)
 - TF(t,d):單詞t在文件d中出現的頻率

 - IDF(t)逆文件頻率:如果一個單詞在非常多文章裡都出現，那可能是一個比較通用的詞彙，對區分抹篇章特殊語意的貢獻較小，因此對權重做出一定懲罰

## 模型評估
### 準確度(accuracy)的侷限
當不同類別的樣本比例非常不均勻，占比較大的類別往往會成為引響準確度的主要因素。

### 精確度(precision)與召回率(recall)的權衡
precision與recall是既矛盾又統一的兩個指標，為了提升precision，分類器需要盡量在 "更有把握 " 時才把樣本預測為正樣本，但此時卻會因 "過於保守" 而漏掉很多沒有把握的正樣本，導致recall降低，而只用某點對應的
precision與recall是不能全面衡量模型的效能，只有透過P-R曲線的整體表現，才能對模型進行更全面的評估

### 平方根誤差的意外
一般情況下，RMSE可以良好的反應回歸模型預測值與真實質的偏離程度，但在真實情況下，如果存在偏離程度非常大的離群值，即使離群點數量非常少，也會讓RMSE指標便非常差。
針對這個問題可以從三個角度解決
- 如果認定離群點是雜訊點，就需要在資料預處理階段就把雜訊點濾掉
 - 如果不認定離群點是"雜訊點"，就需要進一步提升模型的預測能力，
將離群點產生的機制建模進去，
 - 使用更適合的指標來評估模型，如MAPE，相較於RMSE，MAPE把每個點的誤差都進行正規化，降低了離群點帶來的絕對誤差引響。

### ROC曲線
ROC曲線的橫軸為偽陽性率(FPR)，縱軸為真陽性率(TPR)，公式分別為FPR=FP/真實負樣本數，TPR=TP/真實正樣本數

### 如何計算AUC
AUC指ROC曲線底下面積大小，只需沿著ROC的橫軸做積分，AUC越大，說明越能把真正的正樣本排在最前面

### 為什麼一些場景要使用餘弦相似度，而不是歐式距離
整體來說，歐式距離展示了"數值上的差異"，而展示了"方向上的相對差異"，例如
- 統計兩部劇用戶的觀看行為，觀看A劇用戶的向量為(0,1)，觀看B劇用戶的向量為(1,0)，此時餘弦相似度高，而歐式距離很小，分析兩用戶對不同影片偏好，更關注相對差異，應該使用餘弦相似度

- 當分析用戶活躍度時，以登入次數(次)和平均觀看時間(分鐘)作特徵時，餘弦相似度會認為(1,10),(10,100)兩個用戶距離很近，但顯然兩個用戶活躍度有極大差異，此時我們更關注數值絕對差異，故應該使用歐式距離


## 對模型進行線下評估後，為什麼還要進行A/B測試
- 離線評估無法完全解決過擬問題
- 離線評估無法完全還原線上環境，不會考慮到延遲、資料遺失、標籤缺失等問題
- 離線評估與線上系統評估的指標通常不同，如推薦演算法中，上線系統通常評估點擊率、停留時間、瀏覽量等而離線評估則關注ROC、P-R曲線等

## 如何進行A/B測試
A/B測試主要手段是進行用戶分組，將用戶分成實驗組與對照組，對實驗組施以新模型、對照組施以舊模型，分組過程中要注意樣本的獨立性和採樣方式的不偏性，確保同一個用戶每次只能被分到一組中，且選取的userId必須是一個隨機值，才能保證不偏性

## 最佳化演算法

### 訓練資料特別大時梯度下降法的問題與解決
梯度下降法每次更新模型參數時需尋訪所有訓練資料，當訓練資料特別大時，便要求極大的計算量，耗費很長的時間，且實際應用上基本不大可行，為了解決這個問題，**隨機梯度下降法**改用單個訓練樣本的損失來近似平均損失，單個訓練資料時，即可一次更新模型參數，大幅加跨收斂速度，而為了將低隨機梯度的變異數，進而使得迭代演算法更加穩定，並充分利用高度最佳化的矩陣運算操作，實際運用時會同時處理若干訓練資料，
該方法稱為**小批次梯度下降法**

## 演算法

### Lasso 
L1 norm:迴歸模型(可調整「誤差項」與「解釋變數量」)

### ElasticNet彈性網路回歸:
L1 norm與L2 norm綜合版

綜合了Ridge 懲罰項達到有效正規化優勢以及Lasso 懲罰項能夠進行變數挑選優勢，允許學習到一個只有少量引數是非零稀疏的模型鼓勵在高度相關變數的情況下的群體效應
能夠控制multicollinearity的的問題，能夠在p>n時執行回歸，並降低資料中過多的雜訊，以幫助我們將具有影響力的變數獨立出來且維持住模型正確率。

依舊有假設預測變數和目標變數需具有線性關係。雖然我們可以結合non-additive models(一種無母數回歸模型，non-parametric regression)交互作用，但當資料變數很多的時候，會是非常繁瑣與困難的

### KNeighborsRegressor:
預測值為k個最近鄰居的值的平均值

### DecisionTreeRegressor決策樹回歸:
計算熵與不純度

### Support Vector Regression:
讓多維分割線與被分割資料間距離最大化

### AdaBoost自我提升法:
前一個基本分類器分錯的樣本會得到加強，加權後的全體樣本再次被用來訓練下一個基本分類器

### GradientBoostingRegressor梯度提升法
先以弱模型預測。再將先前低度擬合的預測依次增加到集合中，確保錯誤修正

### 隨機森林
構建一棵樹的分裂節點的時候，隨機選取特徵


### 極端隨機森林
構建一棵樹的分裂節點的時候，不會任意地選取特徵；而是先隨機收集一部分特徵，然後利用信息熵和基尼不純性等指標調休最佳的節點特徵

## 推薦演算法

## Content-Based Recommendations
如果這個數據中的每一個項目都有屬於自己的 metadata，它便可以藉由相似的 metadata 標籤推薦項目

## 協同過濾(Collaborative Filtering) 

### Memory-based Collaborative Filtering
據眾人的反饋，來衡量彼此之間的相似度，衡量相似度的維度
### User-based (與你相似的用戶也購買了…)
#### 場景
- Suggestion(Non-Personalized):簡化版的 User-based 推薦，不考慮不同評分間的差異。
#### 優點
- 隨著時間推演，演算法推薦會越來越穩定
- 能夠提前在商品上架前計算演算法矩陣
#### 缺點
- 無法推薦使用者潛在感興趣的商品，容易重複與老套
- 冷啟動問題（Cold Start），且沒有使用者資料便無法推薦
- 資料稀疏問題，需要以 KNN／MF 矩陣分解來解決


### Item-based(購買此商品的人也買了…)
#### 場景:
- Best-seller(Non-Personalized) :商品數不多，銷售差異懸殊，無須太多的個人化推薦。
- Feature Similarity( Personalized):以 Jaccard Similarity 計算相似度。主要以聯集與交集來判斷哪些商品出現的機率較高，但沒有不同商品間評分的差異
#### 優點：
- 可以根據使用者來推薦潛在興趣，變化多樣。
#### 缺點：
- 商品推薦結果容易變化，缺乏穩定性。
- 冷啟動問題（Cold Start），且初期推薦效果非常差。
- 資料稀疏問題，需要以 KNN／MF 矩陣分解來解決。

## 如何在使用者以及物件之間測量相似性
 - Jaccard Similarity(雅卡爾相似度):所有的聯集中，有多少比例是交集。
 - Cosine Similarity(餘弦相似度):兩個物件之間的距離越高，則代表越分開，相似度越小。反過來說，當兩個物件之間的相似度越高，則距離則越接近。通常相似度的指標會用 0、1 來代表，0 表示不相似(完全不一樣)，1 表示相似(完全一樣), 
 - Pearson Similarity(相關係數):兩變量間的相關度，並與平均評分進行修正
 